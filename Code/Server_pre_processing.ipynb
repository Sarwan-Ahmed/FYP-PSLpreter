{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Server pre-processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmpxBrfsz9Rs"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdMdDLf00ApK"
      },
      "source": [
        "def getFrame(sec, vidcap, count):\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
        "    hasFrames, image = vidcap.read()\n",
        "\n",
        "    if hasFrames:\n",
        "        #image = cv2.resize(image, dsize=(120,120))              #resize image to 120x120\n",
        "        filename = '/content/drive/MyDrive/Testing/frames_test/' + \"%d.png\" % count\n",
        "        cv2.imwrite(filename, image)    # save frame as JPG file\n",
        "        print(filename)\n",
        "\n",
        "    return hasFrames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVkYtknq0HL-"
      },
      "source": [
        "def createFrames(video):\n",
        "  \n",
        "  #first remove the existing frames from directory\n",
        "  print('removing frames...')\n",
        "  path = '/content/drive/MyDrive/Testing/frames_test'\n",
        "  files = os.listdir(path)\n",
        "  for f in files:\n",
        "    os.remove(path+'/'+f)\n",
        "    #print(f)\n",
        "  \n",
        "  print('creating frames...')\n",
        "\n",
        "  vidcap = cv2.VideoCapture(video)\n",
        "  sec = 0\n",
        "  frameRate = 1/6  # it will capture image in each 1/6th second\n",
        "  count = 1 \n",
        "  success = getFrame(sec, vidcap, count)\n",
        "\n",
        "  while success:\n",
        "    count = count + 1\n",
        "    sec = sec + frameRate\n",
        "    sec = round(sec, 2)\n",
        "    success = getFrame(sec, vidcap, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_XQ85X6EYMV"
      },
      "source": [
        "#global sentences dictionary\n",
        "sentences_dict = {0: 'None', 1:'آج کیا تاریخ ہے', 2:'پولیس سکول میں جا رہی ہے', 3:'پاکستان میرا پیارا وطن ہے', 4:'واش روم کدھر ہے', 5:'میری دو بہنیں ہیں',\n",
        "                  6:'میرے دو بھائ ہیں', 7:'میں طالب علم ہوں', 8:'مجھے راستہ بھول گیا ہے', 9:'کیا وقت ہوا ہے', 10:'بلی کو بھوک لگی ہے',\n",
        "                  11:'اس بچے کو پانی چاہیے', 12:'آپ کا شکریہ', 13:'آپ سے مل کر اچھا لگا', 14:'مجھے میرے امی ابو سے پیار ہے',\n",
        "                  15:'میرے دوست کو چاکلیٹ پسند ہے', 16:'میری بہن نے ٹرین نہیں دیکھی', 17:'براہ مہربانی مجھے جانے دیں',\n",
        "                  18:'بھائ کمپیوٹر خریدنے کے لیے پیسے دیں', 19:'ہم ملتان سے لاھور ٹرین میں گۓ', 20:'مجھے اب جانا چاہیے'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35kKwXLpCb5N"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import keras\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDPiaeHYlpu6"
      },
      "source": [
        "def toNumpy(dir):\n",
        "    img_array = np.empty([0,43200], np.uint8, 'C')\n",
        "    print('Converting frames to Numpy array...')\n",
        "\n",
        "    dirs = os.listdir(dir)\n",
        "    for img in dirs:\n",
        "      #print(img)\n",
        "      path = dir+\"/\"+img\n",
        "\n",
        "      if path.endswith(\".png\"):\n",
        "        I = cv2.imread(path)\n",
        "        I = cv2.resize(I, dsize=(120,120))\n",
        "        im = (np.array(I))\n",
        "        r = im[:, :, 0].flatten()\n",
        "        g = im[:, :, 1].flatten()\n",
        "        b = im[:, :, 2].flatten()\n",
        "        a = [list(r) + list(g) + list(b)]\n",
        "        b = np.asarray(a)\n",
        "        img_array = np.append(img_array, b, axis=0)\n",
        "    \n",
        "    print('Reshaping array...')\n",
        "    img_array = img_array.reshape(img_array.shape[0], 120, 120, 3)\n",
        "    print('Normalizing array...')\n",
        "    img_array = img_array.astype('float32')\n",
        "    img_array = img_array/ 255.\n",
        "\n",
        "    return img_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBmLVC8SbnxB"
      },
      "source": [
        "def predictSentence(frames_dir):\n",
        "\n",
        "  #preprocess frames\n",
        "  images_array = toNumpy(frames_dir)\n",
        "\n",
        "  #define CNN model\n",
        "  sign_model = Sequential()\n",
        "  sign_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(120,120,3)))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))\n",
        "  sign_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "  sign_model.add(Dropout(0.4))\n",
        "  sign_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))\n",
        "  sign_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "  sign_model.add(Dropout(0.4))\n",
        "  sign_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))                  \n",
        "  sign_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "  sign_model.add(Dropout(0.4))\n",
        "  sign_model.add(Flatten())\n",
        "  sign_model.add(Dense(128, activation='linear'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))           \n",
        "  sign_model.add(Dropout(0.4))\n",
        "  sign_model.add(Dense(21, activation='softmax'))\n",
        "\n",
        "  # loading the trained weights\n",
        "  sign_model.load_weights(\"/content/drive/MyDrive/PSL dataset/CNN_bg_weights.hdf5\")\n",
        "\n",
        "  # compiling the model\n",
        "  sign_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "  \n",
        "  #predict the video\n",
        "  print('Pridicting Sentence...')\n",
        "  predictions = sign_model.predict(images_array)\n",
        "  predictions = np.argmax(np.round(predictions),axis=1)\n",
        "  print(predictions)\n",
        "\n",
        "  #maximum predicted sentence\n",
        "  sentence_key = np.bincount(predictions).argmax()\n",
        "  occurrences = np.count_nonzero(predictions == sentence_key)\n",
        "  print('confidence:', occurrences/len(predictions))\n",
        "\n",
        "  #take sentence from dictionary using predicted key\n",
        "  global sentences_dict\n",
        "  sentence = sentences_dict[sentence_key]\n",
        "\n",
        "  return 'Predicted sentence is: '+sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcB8DGDmdRxn",
        "outputId": "47af0b2d-d7da-4378-aa4f-7cd797168649"
      },
      "source": [
        "createFrames('/content/drive/MyDrive/PSL Schools Dataset/front/Farukh/sentence18.mp4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "removing frames...\n",
            "creating frames...\n",
            "/content/drive/MyDrive/Testing/frames_test/1.png\n",
            "/content/drive/MyDrive/Testing/frames_test/2.png\n",
            "/content/drive/MyDrive/Testing/frames_test/3.png\n",
            "/content/drive/MyDrive/Testing/frames_test/4.png\n",
            "/content/drive/MyDrive/Testing/frames_test/5.png\n",
            "/content/drive/MyDrive/Testing/frames_test/6.png\n",
            "/content/drive/MyDrive/Testing/frames_test/7.png\n",
            "/content/drive/MyDrive/Testing/frames_test/8.png\n",
            "/content/drive/MyDrive/Testing/frames_test/9.png\n",
            "/content/drive/MyDrive/Testing/frames_test/10.png\n",
            "/content/drive/MyDrive/Testing/frames_test/11.png\n",
            "/content/drive/MyDrive/Testing/frames_test/12.png\n",
            "/content/drive/MyDrive/Testing/frames_test/13.png\n",
            "/content/drive/MyDrive/Testing/frames_test/14.png\n",
            "/content/drive/MyDrive/Testing/frames_test/15.png\n",
            "/content/drive/MyDrive/Testing/frames_test/16.png\n",
            "/content/drive/MyDrive/Testing/frames_test/17.png\n",
            "/content/drive/MyDrive/Testing/frames_test/18.png\n",
            "/content/drive/MyDrive/Testing/frames_test/19.png\n",
            "/content/drive/MyDrive/Testing/frames_test/20.png\n",
            "/content/drive/MyDrive/Testing/frames_test/21.png\n",
            "/content/drive/MyDrive/Testing/frames_test/22.png\n",
            "/content/drive/MyDrive/Testing/frames_test/23.png\n",
            "/content/drive/MyDrive/Testing/frames_test/24.png\n",
            "/content/drive/MyDrive/Testing/frames_test/25.png\n",
            "/content/drive/MyDrive/Testing/frames_test/26.png\n",
            "/content/drive/MyDrive/Testing/frames_test/27.png\n",
            "/content/drive/MyDrive/Testing/frames_test/28.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTucNlCxeq3y"
      },
      "source": [
        "#for 90 degree clockwise rotation\n",
        "path = '/content/drive/MyDrive/Testing/frames_test'\n",
        "files = os.listdir(path)\n",
        "for f in files:\n",
        "  src = cv2.imread(path+'/'+f)\n",
        "  \n",
        "  # Using cv2.rotate() method\n",
        "  # Using cv2.ROTATE_90_CLOCKWISE rotate\n",
        "  # by 90 degrees clockwise\n",
        "  image = cv2.rotate(src, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
        "  cv2.imwrite(path+'/'+f, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30PWH5z4IxNj",
        "outputId": "92c6b75b-a10f-4b11-cf37-6302e72cf826"
      },
      "source": [
        "print(predictSentence('/content/drive/MyDrive/Testing/frames_test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting frames to Numpy array...\n",
            "Reshaping array...\n",
            "Normalizing array...\n",
            "Pridicting Sentence...\n",
            "[16 16 16 16 16 16 16 16 16 17  0  0  0 17 17 17 17 17 17 16 16 16 16 16\n",
            " 16 17 16 16  0 15 15  0  0 16 16]\n",
            "confidence: 0.5428571428571428\n",
            "Predicted sentence is: میری بہن نے ٹرین نہیں دیکھی\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WTMEAPMAmlX"
      },
      "source": [
        "def predictSentence(frames_dir):\n",
        "\n",
        "  #preprocess frames\n",
        "  images_array = toNumpy(frames_dir)\n",
        "\n",
        "  #define CNN model (testtest1)\n",
        "  sign_model = Sequential()\n",
        "  sign_model.add(Conv2D(64, kernel_size=(5, 5),activation='linear',padding='same',input_shape=(120,120,3)))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))\n",
        "  sign_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "  #sign_model.add(Dropout(0.5))\n",
        "  sign_model.add(Conv2D(64, (5, 5), activation='linear',padding='same'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))\n",
        "  sign_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "  #sign_model.add(Dropout(0.5))\n",
        "  sign_model.add(Conv2D(64, (5, 5), activation='linear',padding='same'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))                  \n",
        "  sign_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "  #sign_model.add(Dropout(0.5))\n",
        "  sign_model.add(Flatten())\n",
        "  sign_model.add(Dense(256, activation='linear'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))           \n",
        "  #sign_model.add(Dropout(0.5))\n",
        "  sign_model.add(Dense(128, activation='linear'))\n",
        "  sign_model.add(LeakyReLU(alpha=0.1))           \n",
        "  #sign_model.add(Dropout(0.5))\n",
        "  sign_model.add(Dense(21, activation='softmax'))\n",
        "\n",
        "#epochs 735, accuracy 93.4, lr 1e-5\n",
        "\n",
        "  # loading the trained weights\n",
        "  sign_model.load_weights(\"/content/drive/MyDrive/PSL dataset/best_model.hdf5\")\n",
        "\n",
        "  # compiling the model\n",
        "  sign_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "  \n",
        "  #predict the video\n",
        "  print('Pridicting Sentence...')\n",
        "  predictions = sign_model.predict(images_array)\n",
        "  predictions = np.argmax(np.round(predictions),axis=1)\n",
        "  print(predictions)\n",
        "\n",
        "  #maximum predicted sentence\n",
        "  sentence_key = np.bincount(predictions).argmax()\n",
        "  occurrences = np.count_nonzero(predictions == sentence_key)\n",
        "  print('confidence:', occurrences/len(predictions))\n",
        "\n",
        "  #take sentence from dictionary using predicted key\n",
        "  global sentences_dict\n",
        "  sentence = sentences_dict[sentence_key]\n",
        "\n",
        "  return 'Predicted sentence is: '+sentence"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}